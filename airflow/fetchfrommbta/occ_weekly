from datetime import datetime, timezone

from airflow import DAG
from airflow.providers.google.cloud.operators.bigquery import (
    BigQueryInsertJobOperator,
    BigQueryCreateEmptyDatasetOperator,
)

PROJECT = "ba882-f25-class-project-team9"
DS_RT = "mbta_rt"
DS_ML = "mbta_ml"
BQ_LOCATION = "US"

SQL_FEATURES = f"""
CREATE SCHEMA IF NOT EXISTS `{PROJECT}.{DS_ML}`;
CREATE OR REPLACE TABLE `{PROJECT}.{DS_ML}.occ_features_min`
PARTITION BY DATE(ts) CLUSTER BY route_id AS
SELECT
  UPPER(TRIM(occupancy_status)) AS label,
  route_id, current_status,
  EXTRACT(HOUR FROM observed_at_utc) AS hour_of_day,
  EXTRACT(DAYOFWEEK FROM observed_at_utc) AS day_of_week,
  IF(EXTRACT(DAYOFWEEK FROM observed_at_utc) IN (1,7), 1, 0) AS is_weekend,
  ROUND(latitude, 3)  AS lat_bin,
  ROUND(longitude, 3) AS lon_bin,
  observed_at_utc AS ts
FROM `{PROJECT}.{DS_RT}.rt_vehicle_positions`
WHERE occupancy_status IS NOT NULL
  AND latitude BETWEEN 41.5 AND 42.9
  AND longitude BETWEEN -71.9 AND -70.5;
"""

SQL_TRAIN_DATA = f"""
SELECT * EXCEPT(ts)
FROM `{PROJECT}.{DS_ML}.occ_features_min`
WHERE ts <  TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
  AND ts >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 67 DAY)
"""

SQL_TRAIN_LR = f"""
CREATE OR REPLACE MODEL `{PROJECT}.{DS_ML}.occ_lr_min`
OPTIONS (
  MODEL_TYPE = 'LOGISTIC_REG',
  INPUT_LABEL_COLS = ['label'],
  AUTO_CLASS_WEIGHTS = TRUE,
  MAX_ITERATIONS = 50
) AS
{SQL_TRAIN_DATA};
"""

SQL_EVAL_LR = f"""
CREATE OR REPLACE TABLE `{PROJECT}.{DS_ML}.occ_eval_last_lr` AS
SELECT * FROM ML.EVALUATE(
  MODEL `{PROJECT}.{DS_ML}.occ_lr_min`,
  (
    SELECT * EXCEPT(ts)
    FROM `{PROJECT}.{DS_ML}.occ_features_min`
    WHERE ts >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
  )
);
"""

SQL_TRAIN_BT = f"""
CREATE OR REPLACE MODEL `{PROJECT}.{DS_ML}.occ_bt_min`
OPTIONS (
  MODEL_TYPE = 'BOOSTED_TREE_CLASSIFIER',
  INPUT_LABEL_COLS = ['label'],
  AUTO_CLASS_WEIGHTS = TRUE,
  NUM_PARALLEL_TREE = 4,
  MAX_TREE_DEPTH = 6
) AS
{SQL_TRAIN_DATA};
"""

SQL_EVAL_BT = f"""
CREATE OR REPLACE TABLE `{PROJECT}.{DS_ML}.occ_eval_last_bt` AS
SELECT * FROM ML.EVALUATE(
  MODEL `{PROJECT}.{DS_ML}.occ_bt_min`,
  (
    SELECT * EXCEPT(ts)
    FROM `{PROJECT}.{DS_ML}.occ_features_min`
    WHERE ts >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
  )
);
"""

SQL_TRAIN_DNN = f"""
CREATE OR REPLACE MODEL `{PROJECT}.{DS_ML}.occ_dnn_min`
OPTIONS (
  MODEL_TYPE = 'DNN_CLASSIFIER',
  INPUT_LABEL_COLS = ['label'],
  HIDDEN_UNITS = [64, 32]
) AS
{SQL_TRAIN_DATA};
"""

SQL_EVAL_DNN = f"""
CREATE OR REPLACE TABLE `{PROJECT}.{DS_ML}.occ_eval_last_dnn` AS
SELECT * FROM ML.EVALUATE(
  MODEL `{PROJECT}.{DS_ML}.occ_dnn_min`,
  (
    SELECT * EXCEPT(ts)
    FROM `{PROJECT}.{DS_ML}.occ_features_min`
    WHERE ts >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
  )
);
"""

SQL_EVAL_SUMMARY = f"""
CREATE OR REPLACE TABLE `{PROJECT}.{DS_ML}.occ_eval_all_models` AS
SELECT 'lr' AS model, * FROM `{PROJECT}.{DS_ML}.occ_eval_last_lr`
UNION ALL
SELECT 'bt' AS model, * FROM `{PROJECT}.{DS_ML}.occ_eval_last_bt`
UNION ALL
SELECT 'dnn' AS model, * FROM `{PROJECT}.{DS_ML}.occ_eval_last_dnn`;
"""

with DAG(
    dag_id="occ_train_weekly",
    start_date=datetime(2025, 1, 1, tzinfo=timezone.utc),
    schedule="0 5 * * 1",
    catchup=False,
    tags=["mbta", "occ", "train"],
) as dag_occ_train:
    create_ml_dataset = BigQueryCreateEmptyDatasetOperator(
        task_id="create_ml_dataset",
        project_id=PROJECT,
        dataset_id=DS_ML,
        location=BQ_LOCATION,
        exists_ok=True,
        gcp_conn_id="google_cloud_default",
    )

    features = BigQueryInsertJobOperator(
        task_id="build_features",
        configuration={"query": {"query": SQL_FEATURES, "useLegacySql": False}},
        gcp_conn_id="google_cloud_default",
        location=BQ_LOCATION,
    )
    train_lr = BigQueryInsertJobOperator(
        task_id="train_lr_model",
        configuration={"query": {"query": SQL_TRAIN_LR, "useLegacySql": False}},
        gcp_conn_id="google_cloud_default",
        location=BQ_LOCATION,
    )

    train_bt = BigQueryInsertJobOperator(
        task_id="train_bt_model",
        configuration={"query": {"query": SQL_TRAIN_BT, "useLegacySql": False}},
        gcp_conn_id="google_cloud_default",
        location=BQ_LOCATION,
    )

    train_dnn = BigQueryInsertJobOperator(
        task_id="train_dnn_model",
        configuration={"query": {"query": SQL_TRAIN_DNN, "useLegacySql": False}},
        gcp_conn_id="google_cloud_default",
        location=BQ_LOCATION,
    )

    eval_lr = BigQueryInsertJobOperator(
        task_id="evaluate_lr",
        configuration={"query": {"query": SQL_EVAL_LR, "useLegacySql": False}},
        gcp_conn_id="google_cloud_default",
        location=BQ_LOCATION,
    )

    eval_bt = BigQueryInsertJobOperator(
        task_id="evaluate_bt",
        configuration={"query": {"query": SQL_EVAL_BT, "useLegacySql": False}},
        gcp_conn_id="google_cloud_default",
        location=BQ_LOCATION,
    )

    eval_dnn = BigQueryInsertJobOperator(
        task_id="evaluate_dnn",
        configuration={"query": {"query": SQL_EVAL_DNN, "useLegacySql": False}},
        gcp_conn_id="google_cloud_default",
        location=BQ_LOCATION,
    )

    eval_summary = BigQueryInsertJobOperator(
        task_id="evaluate_summary",
        configuration={"query": {"query": SQL_EVAL_SUMMARY, "useLegacySql": False}},
        gcp_conn_id="google_cloud_default",
        location=BQ_LOCATION,
    )

    create_ml_dataset >> features >> [train_lr, train_bt, train_dnn]
    train_lr >> eval_lr
    train_bt >> eval_bt
    train_dnn >> eval_dnn
    [eval_lr, eval_bt, eval_dnn] >> eval_summary

